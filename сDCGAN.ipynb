{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programming\\Anaconda1\\envs\\NN\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from datetime import date\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "from matplotlib.image import imread\n",
    "from scipy.misc.pilutil import imresize, imsave\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Reshape, Flatten, Dropout, Input\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, Conv2DTranspose, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.datasets import mnist\n",
    "from keras import initializers\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    \n",
    "        model = Sequential()\n",
    "        model.add(Dense(128 * 16 * 8, input_dim = latent_dim))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Reshape((8, 8, 256)))\n",
    "        model.add(Conv2DTranspose(512, filter_size_g, strides=(1,1), padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Conv2DTranspose(512, filter_size_g, strides=(1,1), padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Conv2DTranspose(256, filter_size_g, strides=(1,1), padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Conv2DTranspose(128, filter_size_g, strides=(2,2), padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Conv2DTranspose(64, filter_size_g, strides=(2,2), padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Conv2DTranspose(32, filter_size_g, strides=(2,2), padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Conv2DTranspose(16, filter_size_g, strides=(2,2), padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Conv2DTranspose(8, filter_size_g, strides=(2,2), padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Conv2DTranspose(img_channels, filter_size_g, strides=(1,1), padding='same'))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, kernel_size=filter_size_d, strides = (2,2), input_shape=img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(128, kernel_size=filter_size_d, strides = (2,2), padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(256,  kernel_size=filter_size_d, strides = (2,2), padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(512, kernel_size=filter_size_d, strides = (2,2), padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(512, kernel_size=filter_size_d, strides = (2,2), padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Conv2D(512, kernel_size=filter_size_d, strides = (2,2), padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1))\n",
    "        model.add(Activation('sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_containing_discriminator(g, d):\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    img = g(noise)\n",
    "    d.trainable = False\n",
    "    #d_label.trainable = False\n",
    "    valid = d(img)\n",
    "    \n",
    "    \n",
    "    return Model(noise, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_imgs(epoch, gen):\n",
    "        count = 5\n",
    "        noise = np.random.uniform(-1, 1, size=(count, latent_dim))\n",
    "        gen_imgs = gen.predict(noise)\n",
    "\n",
    "        gen_imgs = 127.5 * gen_imgs + 127.5\n",
    "        \n",
    "        if gen_imgs.shape[3] == 1:\n",
    "            gen_imgs = gen_imgs[:,:,:,0]\n",
    "        \n",
    "        for i in range(count):\n",
    "            cv2.imwrite(os.getcwd() + '\\\\generated\\\\epoch%d_%d.png' % (epoch, i), gen_imgs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(indices, data, batch_size):\n",
    "    X_train = np.zeros((batch_size, img_rows, img_cols, img_channels))\n",
    "    for i in range(batch_size):\n",
    "        if color_mode == 'grayscale':\n",
    "            temp_img = cv2.imread(data[indices[i]], 0)\n",
    "            X_train[i,:,:,0] = temp_img\n",
    "        else:\n",
    "            temp_img = cv2.imread(data[indices[i]])\n",
    "            X_train[i] = temp_img\n",
    "    X_train = (X_train - 127.5)/127.5\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_classes(batch_size, data):\n",
    "    X_train = np.zeros((batch_size, img_rows, img_cols, img_channels))\n",
    "    y_labels = np.zeros(batch_size)\n",
    "    choice_arr = np.random.randint(0, len(data), batch_size)\n",
    "    for i in range(batch_size):\n",
    "        rand_number = np.random.randint(0, len(data[choice_arr[i]]))\n",
    "        temp_img = cv2.imread(data[choice_arr[i]][rand_number])\n",
    "        \n",
    "        \n",
    "        X_train[i] = temp_img\n",
    "        y_labels[i] = choice_arr[i]\n",
    "    \n",
    "\n",
    "    X_train = (X_train - 127.5)/127.5\n",
    "    return X_train, y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_one_class(batch_size, data, class_target):\n",
    "    X_train = np.zeros((batch_size, img_rows, img_cols, img_channels))\n",
    "    y_labels = np.zeros(batch_size) + class_target\n",
    "    '''choice_arr = np.random.randint(0, len(data[class_target]), batch_size)\n",
    "    for i in range(batch_size):\n",
    "        temp_img = cv2.imread(data[class_target][choice_arr[i]])\n",
    "\n",
    "        X_train[i] = temp_img\n",
    "        y_labels[i] = choice_arr[i]'''\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        temp_img = cv2.imread(os.getcwd() + '/new256_images/abstraktnyy-ekspressionizm/303.png')\n",
    "        \n",
    "        X_train[i] = temp_img\n",
    "        y_labels[i] = 0\n",
    "    X_train = (X_train - 127.5)/127.5\n",
    "    return X_train, y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num)/width))\n",
    "    shape = generated_images.shape[1:3]\n",
    "    image = np.zeros((height*shape[0], width*shape[1], img_channels),\n",
    "                     dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
    "            img[:, :, :,]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    styles_folder = os.listdir(path=os.getcwd() + \"\\\\new256_images\\\\\")\n",
    "    num_styles = len(styles_folder)\n",
    "    data = []\n",
    "    for i in range(num_styles):\n",
    "        data.append(glob.glob(os.getcwd() + '\\\\new256_images\\\\' + styles_folder[i] + '\\\\*'))\n",
    "    return data, num_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_another(epochs = 100, BATCH_SIZE = 4):\n",
    "\n",
    "    data, num_styles = get_data()\n",
    "    \n",
    "    generator = build_generator()\n",
    "    discriminator = build_discriminator()\n",
    "    \n",
    "    discriminator.compile(loss=losses[0], optimizer=d_optim)\n",
    "    #d_label.compile(loss=losses[1], optimizer=d_optim)\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "    \n",
    "    dcgan = generator_containing_discriminator(generator, discriminator)\n",
    "    \n",
    "    \n",
    "    dcgan.compile(loss=losses[0], optimizer=g_optim)\n",
    "    \n",
    "    discriminator.trainable = True\n",
    "    #d_label.trainable = True\n",
    "    \n",
    "    \n",
    "    '''(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "    X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))'''\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for index in range(int(15000/BATCH_SIZE)):\n",
    "            noise = np.random.normal(0, 1, (BATCH_SIZE, latent_dim))\n",
    "                \n",
    "            #real_images, real_labels = get_images_classes(batch_size=BATCH_SIZE, data = data)\n",
    "            \n",
    "            label = index % num_styles\n",
    "            \n",
    "            real_images, real_labels = get_images_one_class(BATCH_SIZE, data, label)\n",
    "            \n",
    "            real_images += np.random.normal(size = img_shape, scale= 0.1)\n",
    "            \n",
    "            generated_images = generator.predict(noise)\n",
    "            \n",
    "            if index % 2 == 0:\n",
    "                X = real_images\n",
    "                real_labels = real_labels - 0.1 + np.random.rand(BATCH_SIZE)*0.2\n",
    "                y_classif = keras.utils.to_categorical(np.zeros(BATCH_SIZE) + label, num_styles)\n",
    "                y = 0.8 + np.random.rand(BATCH_SIZE)*0.2\n",
    "                \n",
    "                d_loss = []\n",
    "                d_loss.append(discriminator.train_on_batch(X, y))\n",
    "                #discriminator.trainable = False\n",
    "                #d_loss.append(d_label.train_on_batch(X, y_classif))\n",
    "                print(\"epoch %d batch %d d_loss : %f\" % (epoch, index, d_loss[0]))\n",
    "            else:\n",
    "                X = generated_images\n",
    "                y = np.random.rand(BATCH_SIZE) * 0.2\n",
    "                d_loss = discriminator.train_on_batch(X, y)\n",
    "                print(\"epoch %d batch %d d_loss : %f\" % (epoch, index, d_loss))\n",
    "            \n",
    "            \n",
    "            noise = np.random.normal(0, 1, (BATCH_SIZE, latent_dim))\n",
    "            \n",
    "            discriminator.trainable = False\n",
    "            #d_label.trainable = False\n",
    "            \n",
    "            y_classif = keras.utils.to_categorical(np.zeros(BATCH_SIZE) + 0.5, num_styles)\n",
    "            y = np.random.rand(BATCH_SIZE) * 0.3\n",
    "            \n",
    "            g_loss = dcgan.train_on_batch(noise, y)\n",
    "            \n",
    "            #d_label.trainable = True\n",
    "            discriminator.trainable = True\n",
    "            \n",
    "            print(\"epoch %d batch %d g_loss : %f\" % (epoch, index, g_loss))\n",
    "            \n",
    "            if index % 50 == 0:\n",
    "                        image = combine_images(generated_images)\n",
    "                        image = image*127.5+127.5\n",
    "                        cv2.imwrite(\n",
    "                            os.getcwd() + '\\\\generated\\\\epoch%d_%d.png' % (epoch, index), image)\n",
    "                        image = combine_images(real_images)\n",
    "                        image = image*127.5+127.5\n",
    "                        cv2.imwrite(\n",
    "                            os.getcwd() + '\\\\generated\\\\epoch%d_%d_data.png' % (epoch, index), image)\n",
    "                        \n",
    "        if epoch % 5 == 0:\n",
    "            \n",
    "            \n",
    "            date_today = date.today()\n",
    "            \n",
    "            \n",
    "            \n",
    "            month, day = date_today.month, date_today.day\n",
    "            \n",
    "            # Генерируем описание модели в формате json\n",
    "            d_json = discriminator.to_json()\n",
    "            # Записываем модель в файл\n",
    "            json_file = open(os.getcwd() + \"%d.%d dis_model.json\" % (day, month), \"w\")\n",
    "            json_file.write(d_json)\n",
    "            json_file.close()\n",
    "            \n",
    "            '''# Генерируем описание модели в формате json\n",
    "            d_l_json = d_label.to_json()\n",
    "            # Записываем модель в файл\n",
    "            json_file = open(os.getcwd() + \"%d.%d dis_label_model.json\" % (day, month), \"w\")\n",
    "            json_file.write(d_l_json)\n",
    "            json_file.close()'''\n",
    "            \n",
    "            # Генерируем описание модели в формате json\n",
    "            gen_json = generator.to_json()\n",
    "            # Записываем модель в файл\n",
    "            json_file = open(os.getcwd() + \"%d.%d gen_model.json\" % (day, month), \"w\")\n",
    "            json_file.write(gen_json)\n",
    "            json_file.close()\n",
    "            \n",
    "            discriminator.save_weights(os.getcwd() + '%d.%d dis_weights.h5' % (day, month))\n",
    "            #d_label.save_weights(os.getcwd() + '%d.%d dis_label_weights.h5' % (day, month))\n",
    "            generator.save_weights(os.getcwd() + '%d.%d gen_weights.h5' % (day, month))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 256\n",
    "img_cols = 256\n",
    "img_channels = 3\n",
    "img_shape = (img_rows, img_cols, img_channels)\n",
    "latent_dim = 100\n",
    "filter_size_g = (5,5)\n",
    "filter_size_d = (5,5)\n",
    "d_strides = (2,2)\n",
    "\n",
    "color_mode = 'rgb'\n",
    "\n",
    "losses = ['binary_crossentropy', 'categorical_crossentropy']\n",
    "\n",
    "\n",
    "#g_optim = SGD(lr = 0.001, momentum=0.9, nesterov=True)\n",
    "#d_optim = SGD(lr = 0.00025, momentum=0.9, nesterov=True)\n",
    "g_optim = Adam(0.0002, beta_2 = 0.5)\n",
    "d_optim = Adam(0.00015, beta_2 = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16384)             1654784   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16384)             65536     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 8, 8, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 8, 8, 512)         6554112   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 8, 8, 256)         3277056   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 16, 16, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 32, 32, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 64, 64, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTr (None, 128, 128, 16)      12816     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 128, 128, 16)      64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 128, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTr (None, 256, 256, 8)       3208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 256, 256, 8)       32        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 256, 256, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_9 (Conv2DTr (None, 256, 256, 3)       603       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256, 256, 3)       0         \n",
      "=================================================================\n",
      "Total params: 15,926,963\n",
      "Trainable params: 15,891,139\n",
      "Non-trainable params: 35,824\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 64)      4864      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 256)       819456    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 512)       3277312   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 512)         6554112   \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 512)         6554112   \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 8193      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 17,430,657\n",
      "Trainable params: 17,426,817\n",
      "Non-trainable params: 3,840\n",
      "_________________________________________________________________\n",
      "epoch 0 batch 0 d_loss : 1.210619\n",
      "epoch 0 batch 0 g_loss : 2.149499\n",
      "epoch 0 batch 1 d_loss : 1.203312\n",
      "epoch 0 batch 1 g_loss : 0.509183\n",
      "epoch 0 batch 2 d_loss : 1.515573\n",
      "epoch 0 batch 2 g_loss : 1.222734\n",
      "epoch 0 batch 3 d_loss : 1.004191\n",
      "epoch 0 batch 3 g_loss : 1.901991\n",
      "epoch 0 batch 4 d_loss : 1.872049\n",
      "epoch 0 batch 4 g_loss : 1.561525\n",
      "epoch 0 batch 5 d_loss : 0.680441\n",
      "epoch 0 batch 5 g_loss : 0.598713\n",
      "epoch 0 batch 6 d_loss : 1.723749\n",
      "epoch 0 batch 6 g_loss : 4.508212\n",
      "epoch 0 batch 7 d_loss : 3.069518\n",
      "epoch 0 batch 7 g_loss : 0.789566\n",
      "epoch 0 batch 8 d_loss : 1.812643\n",
      "epoch 0 batch 8 g_loss : 2.321072\n",
      "epoch 0 batch 9 d_loss : 1.219707\n",
      "epoch 0 batch 9 g_loss : 1.821839\n",
      "epoch 0 batch 10 d_loss : 1.691906\n",
      "epoch 0 batch 10 g_loss : 2.240390\n",
      "epoch 0 batch 11 d_loss : 1.777814\n",
      "epoch 0 batch 11 g_loss : 2.259665\n",
      "epoch 0 batch 12 d_loss : 1.456303\n",
      "epoch 0 batch 12 g_loss : 2.992298\n",
      "epoch 0 batch 13 d_loss : 2.037935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 batch 13 g_loss : 2.455049\n",
      "epoch 0 batch 14 d_loss : 1.338524\n",
      "epoch 0 batch 14 g_loss : 2.873648\n",
      "epoch 0 batch 15 d_loss : 1.848513\n",
      "epoch 0 batch 15 g_loss : 2.675579\n",
      "epoch 0 batch 16 d_loss : 1.862303\n",
      "epoch 0 batch 16 g_loss : 2.397074\n",
      "epoch 0 batch 17 d_loss : 1.474163\n",
      "epoch 0 batch 17 g_loss : 2.085907\n",
      "epoch 0 batch 18 d_loss : 0.971394\n",
      "epoch 0 batch 18 g_loss : 2.951870\n",
      "epoch 0 batch 19 d_loss : 1.399811\n",
      "epoch 0 batch 19 g_loss : 2.869105\n",
      "epoch 0 batch 20 d_loss : 14.731997\n",
      "epoch 0 batch 20 g_loss : 2.388695\n",
      "epoch 0 batch 21 d_loss : 1.289667\n",
      "epoch 0 batch 21 g_loss : 2.070830\n",
      "epoch 0 batch 22 d_loss : 14.228282\n",
      "epoch 0 batch 22 g_loss : 2.781833\n",
      "epoch 0 batch 23 d_loss : 1.855392\n",
      "epoch 0 batch 23 g_loss : 1.951175\n",
      "epoch 0 batch 24 d_loss : 14.605075\n",
      "epoch 0 batch 24 g_loss : 1.867557\n",
      "epoch 0 batch 25 d_loss : 1.804876\n",
      "epoch 0 batch 25 g_loss : 1.658250\n",
      "epoch 0 batch 26 d_loss : 14.758163\n",
      "epoch 0 batch 26 g_loss : 1.044954\n",
      "epoch 0 batch 27 d_loss : 0.608610\n",
      "epoch 0 batch 27 g_loss : 0.734836\n",
      "epoch 0 batch 28 d_loss : 14.372898\n",
      "epoch 0 batch 28 g_loss : 1.432821\n",
      "epoch 0 batch 29 d_loss : 0.948324\n",
      "epoch 0 batch 29 g_loss : 1.307510\n",
      "epoch 0 batch 30 d_loss : 14.498797\n",
      "epoch 0 batch 30 g_loss : 1.091937\n",
      "epoch 0 batch 31 d_loss : 0.638510\n",
      "epoch 0 batch 31 g_loss : 0.597149\n",
      "epoch 0 batch 32 d_loss : 14.570101\n",
      "epoch 0 batch 32 g_loss : 0.545175\n",
      "epoch 0 batch 33 d_loss : 0.474574\n",
      "epoch 0 batch 33 g_loss : 0.469180\n",
      "epoch 0 batch 34 d_loss : 14.222788\n",
      "epoch 0 batch 34 g_loss : 0.591140\n",
      "epoch 0 batch 35 d_loss : 0.431122\n",
      "epoch 0 batch 35 g_loss : 0.463285\n",
      "epoch 0 batch 36 d_loss : 14.438533\n",
      "epoch 0 batch 36 g_loss : 0.438869\n",
      "epoch 0 batch 37 d_loss : 0.418575\n",
      "epoch 0 batch 37 g_loss : 0.399759\n",
      "epoch 0 batch 38 d_loss : 14.540899\n",
      "epoch 0 batch 38 g_loss : 0.577072\n",
      "epoch 0 batch 39 d_loss : 0.438039\n",
      "epoch 0 batch 39 g_loss : 0.486855\n",
      "epoch 0 batch 40 d_loss : 14.456741\n",
      "epoch 0 batch 40 g_loss : 0.372869\n",
      "epoch 0 batch 41 d_loss : 0.386172\n",
      "epoch 0 batch 41 g_loss : 0.487853\n",
      "epoch 0 batch 42 d_loss : 14.549692\n",
      "epoch 0 batch 42 g_loss : 0.446129\n",
      "epoch 0 batch 43 d_loss : 0.283648\n",
      "epoch 0 batch 43 g_loss : 0.450395\n",
      "epoch 0 batch 44 d_loss : 14.847962\n",
      "epoch 0 batch 44 g_loss : 0.462354\n",
      "epoch 0 batch 45 d_loss : 0.322924\n",
      "epoch 0 batch 45 g_loss : 0.454024\n",
      "epoch 0 batch 46 d_loss : 14.494183\n",
      "epoch 0 batch 46 g_loss : 0.454029\n",
      "epoch 0 batch 47 d_loss : 0.356227\n",
      "epoch 0 batch 47 g_loss : 0.426727\n",
      "epoch 0 batch 48 d_loss : 14.717107\n",
      "epoch 0 batch 48 g_loss : 0.522164\n",
      "epoch 0 batch 49 d_loss : 0.440274\n",
      "epoch 0 batch 49 g_loss : 0.546802\n",
      "epoch 0 batch 50 d_loss : 14.746643\n",
      "epoch 0 batch 50 g_loss : 0.583422\n",
      "epoch 0 batch 51 d_loss : 0.379838\n",
      "epoch 0 batch 51 g_loss : 0.666099\n",
      "epoch 0 batch 52 d_loss : 14.140770\n",
      "epoch 0 batch 52 g_loss : 0.930205\n",
      "epoch 0 batch 53 d_loss : 0.525170\n",
      "epoch 0 batch 53 g_loss : 0.639892\n",
      "epoch 0 batch 54 d_loss : 14.470898\n",
      "epoch 0 batch 54 g_loss : 0.502510\n",
      "epoch 0 batch 55 d_loss : 0.437487\n",
      "epoch 0 batch 55 g_loss : 0.542540\n",
      "epoch 0 batch 56 d_loss : 14.541827\n",
      "epoch 0 batch 56 g_loss : 0.620352\n",
      "epoch 0 batch 57 d_loss : 0.370887\n",
      "epoch 0 batch 57 g_loss : 0.547405\n",
      "epoch 0 batch 58 d_loss : 14.777122\n",
      "epoch 0 batch 58 g_loss : 0.429519\n",
      "epoch 0 batch 59 d_loss : 0.393756\n",
      "epoch 0 batch 59 g_loss : 0.431932\n",
      "epoch 0 batch 60 d_loss : 14.440979\n",
      "epoch 0 batch 60 g_loss : 0.617879\n",
      "epoch 0 batch 61 d_loss : 0.417007\n",
      "epoch 0 batch 61 g_loss : 0.483514\n",
      "epoch 0 batch 62 d_loss : 14.277167\n",
      "epoch 0 batch 62 g_loss : 0.543635\n",
      "epoch 0 batch 63 d_loss : 0.401313\n",
      "epoch 0 batch 63 g_loss : 0.485224\n",
      "epoch 0 batch 64 d_loss : 14.804655\n",
      "epoch 0 batch 64 g_loss : 0.506384\n",
      "epoch 0 batch 65 d_loss : 0.389213\n",
      "epoch 0 batch 65 g_loss : 0.528037\n",
      "epoch 0 batch 66 d_loss : 14.640973\n",
      "epoch 0 batch 66 g_loss : 0.591921\n",
      "epoch 0 batch 67 d_loss : 0.400892\n",
      "epoch 0 batch 67 g_loss : 0.440322\n",
      "epoch 0 batch 68 d_loss : 14.222631\n",
      "epoch 0 batch 68 g_loss : 0.559709\n",
      "epoch 0 batch 69 d_loss : 0.399576\n",
      "epoch 0 batch 69 g_loss : 0.562706\n",
      "epoch 0 batch 70 d_loss : 14.083310\n",
      "epoch 0 batch 70 g_loss : 0.545856\n",
      "epoch 0 batch 71 d_loss : 0.376663\n",
      "epoch 0 batch 71 g_loss : 0.582661\n",
      "epoch 0 batch 72 d_loss : 14.219861\n",
      "epoch 0 batch 72 g_loss : 0.692105\n",
      "epoch 0 batch 73 d_loss : 0.407079\n",
      "epoch 0 batch 73 g_loss : 0.691358\n",
      "epoch 0 batch 74 d_loss : 14.673319\n",
      "epoch 0 batch 74 g_loss : 0.825919\n",
      "epoch 0 batch 75 d_loss : 0.358555\n",
      "epoch 0 batch 75 g_loss : 0.713107\n",
      "epoch 0 batch 76 d_loss : 14.358753\n",
      "epoch 0 batch 76 g_loss : 0.540172\n",
      "epoch 0 batch 77 d_loss : 0.276845\n",
      "epoch 0 batch 77 g_loss : 0.592998\n",
      "epoch 0 batch 78 d_loss : 14.472425\n",
      "epoch 0 batch 78 g_loss : 0.482900\n",
      "epoch 0 batch 79 d_loss : 0.366305\n",
      "epoch 0 batch 79 g_loss : 0.439012\n",
      "epoch 0 batch 80 d_loss : 15.093624\n",
      "epoch 0 batch 80 g_loss : 0.461377\n",
      "epoch 0 batch 81 d_loss : 0.341079\n",
      "epoch 0 batch 81 g_loss : 0.593401\n",
      "epoch 0 batch 82 d_loss : 14.764647\n",
      "epoch 0 batch 82 g_loss : 0.483850\n",
      "epoch 0 batch 83 d_loss : 0.376128\n",
      "epoch 0 batch 83 g_loss : 0.400259\n",
      "epoch 0 batch 84 d_loss : 14.631842\n",
      "epoch 0 batch 84 g_loss : 0.478065\n",
      "epoch 0 batch 85 d_loss : 0.457563\n",
      "epoch 0 batch 85 g_loss : 0.549996\n",
      "epoch 0 batch 86 d_loss : 13.999787\n",
      "epoch 0 batch 86 g_loss : 0.527014\n",
      "epoch 0 batch 87 d_loss : 0.222462\n",
      "epoch 0 batch 87 g_loss : 0.499953\n",
      "epoch 0 batch 88 d_loss : 14.307081\n",
      "epoch 0 batch 88 g_loss : 0.434544\n",
      "epoch 0 batch 89 d_loss : 0.354034\n",
      "epoch 0 batch 89 g_loss : 0.411132\n",
      "epoch 0 batch 90 d_loss : 14.619874\n",
      "epoch 0 batch 90 g_loss : 0.472560\n",
      "epoch 0 batch 91 d_loss : 0.338296\n",
      "epoch 0 batch 91 g_loss : 0.514198\n",
      "epoch 0 batch 92 d_loss : 14.276282\n",
      "epoch 0 batch 92 g_loss : 0.561972\n",
      "epoch 0 batch 93 d_loss : 0.377055\n",
      "epoch 0 batch 93 g_loss : 0.545316\n",
      "epoch 0 batch 94 d_loss : 14.702548\n",
      "epoch 0 batch 94 g_loss : 0.489843\n",
      "epoch 0 batch 95 d_loss : 0.389620\n",
      "epoch 0 batch 95 g_loss : 0.463827\n",
      "epoch 0 batch 96 d_loss : 14.390706\n",
      "epoch 0 batch 96 g_loss : 0.565206\n",
      "epoch 0 batch 97 d_loss : 0.452895\n",
      "epoch 0 batch 97 g_loss : 0.516378\n",
      "epoch 0 batch 98 d_loss : 14.781137\n",
      "epoch 0 batch 98 g_loss : 0.464110\n",
      "epoch 0 batch 99 d_loss : 0.370879\n",
      "epoch 0 batch 99 g_loss : 0.449632\n"
     ]
    }
   ],
   "source": [
    "train_another(1000, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_moment = datetime.now()\n",
    "\n",
    "day, month = str(current_moment.day), str(current_moment.month)\n",
    "hour, minute = str(current_moment.hour), str(current_moment.minute)\n",
    "\n",
    "with open(os.getcwd() + '/Changes.txt', 'a') as f:\n",
    "    clock =  (day + '.' + month + '    ' + hour + ':' + minute)\n",
    "    f.write(clock + '...........g_optim -> 0.0003, d_optim->0.0004' + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
