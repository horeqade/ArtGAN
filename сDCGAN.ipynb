{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programming\\Anaconda1\\envs\\NN\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from datetime import date\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "from matplotlib.image import imread\n",
    "from scipy.misc.pilutil import imresize, imsave\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Reshape, Flatten, Dropout, Input\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, Conv2DTranspose, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.datasets import mnist\n",
    "from keras import initializers\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    \n",
    "        model = Sequential()\n",
    "        model.add(Dense(128 * 16 * 8, input_dim = latent_dim))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Reshape((8, 8, 256)))\n",
    "        \n",
    "        model.add(Conv2DTranspose(512, filter_size_g, strides=(1,1), padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU())\n",
    "        \n",
    "        model.add(Conv2DTranspose(512, filter_size_g, strides=(1,1), padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU())\n",
    "        \n",
    "        model.add(Conv2DTranspose(256, filter_size_g, strides=(1,1), padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU())\n",
    "        \n",
    "        model.add(Conv2DTranspose(128, filter_size_g, strides=(2,2), padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU())\n",
    "        \n",
    "        model.add(Conv2DTranspose(64, filter_size_g, strides=(2,2), padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU())\n",
    "        \n",
    "        model.add(Conv2DTranspose(32, filter_size_g, strides=(2,2), padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU())\n",
    "        \n",
    "        model.add(Conv2DTranspose(16, filter_size_g, strides=(2,2), padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU())\n",
    "        \n",
    "        model.add(Conv2DTranspose(8, filter_size_g, strides=(2,2), padding='same'))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU())\n",
    "        \n",
    "        model.add(Conv2DTranspose(img_channels, filter_size_g, strides=(1,1), padding='same'))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(num_classes):\n",
    "\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Conv2D(64, kernel_size=filter_size_d, strides = (2,2), input_shape=img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Conv2D(128, kernel_size=filter_size_d, strides = (2,2), padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Conv2D(256,  kernel_size=filter_size_d, strides = (2,2), padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Conv2D(512, kernel_size=filter_size_d, strides = (2,2), padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Conv2D(512, kernel_size=filter_size_d, strides = (2,2), padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Conv2D(512, kernel_size=filter_size_d, strides = (2,2), padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "        img = Input(shape=img_shape)\n",
    "\n",
    "        features = model(img)\n",
    "\n",
    "        validity = Dense(1)(features)\n",
    "        valid = Activation('sigmoid')(validity)\n",
    "        \n",
    "        label1 = Dense(512)(features)\n",
    "        lrelu1 = LeakyReLU(alpha=0.2)(label1)\n",
    "        label2 = Dense(256)(label1)\n",
    "        lrelu2 = LeakyReLU(alpha=0.2)(label2)\n",
    "        label3 = Dense(num_classes)(label2)\n",
    "        label = Activation('softmax')(label3)\n",
    "        \n",
    "        return Model(img, valid), Model(img, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_containing_discriminator(g, d, d_label):\n",
    "    noise = Input(shape=(latent_dim,))\n",
    "    img = g(noise)\n",
    "    d.trainable = False\n",
    "    d_label.trainable = False\n",
    "    valid, target_label = d(img), d_label(img)\n",
    "    \n",
    "    \n",
    "    return Model(noise, [valid, target_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_imgs(epoch, gen):\n",
    "        count = 5\n",
    "        noise = np.random.uniform(-1, 1, size=(count, latent_dim))\n",
    "        gen_imgs = gen.predict(noise)\n",
    "\n",
    "        gen_imgs = 127.5 * gen_imgs + 127.5\n",
    "        \n",
    "        if gen_imgs.shape[3] == 1:\n",
    "            gen_imgs = gen_imgs[:,:,:,0]\n",
    "        \n",
    "        for i in range(count):\n",
    "            cv2.imwrite(os.getcwd() + '\\\\generated\\\\epoch%d_%d.png' % (epoch, i), gen_imgs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(indices, data, batch_size):\n",
    "    X_train = np.zeros((batch_size, img_rows, img_cols, img_channels))\n",
    "    for i in range(batch_size):\n",
    "        if color_mode == 'grayscale':\n",
    "            temp_img = cv2.imread(data[indices[i]], 0)\n",
    "            X_train[i,:,:,0] = temp_img\n",
    "        else:\n",
    "            temp_img = cv2.imread(data[indices[i]])\n",
    "            X_train[i] = temp_img\n",
    "    X_train = (X_train - 127.5)/127.5\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_classes(batch_size, data):\n",
    "    X_train = np.zeros((batch_size, img_rows, img_cols, img_channels))\n",
    "    y_labels = np.zeros(batch_size)\n",
    "    choice_arr = np.random.randint(0, len(data), batch_size)\n",
    "    for i in range(batch_size):\n",
    "        rand_number = np.random.randint(0, len(data[choice_arr[i]]))\n",
    "        temp_img = cv2.imread(data[choice_arr[i]][rand_number])\n",
    "        \n",
    "        \n",
    "        X_train[i] = temp_img\n",
    "        y_labels[i] = choice_arr[i]\n",
    "    \n",
    "\n",
    "    X_train = (X_train - 127.5)/127.5\n",
    "    return X_train, y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_one_class(batch_size, data, class_target):\n",
    "    X_train = np.zeros((batch_size, img_rows, img_cols, img_channels))\n",
    "    y_label = np.zeros(batch_size)\n",
    "    '''choice_arr = np.random.randint(0, len(data[class_target]), batch_size)\n",
    "    for i in range(batch_size):\n",
    "        temp_img = cv2.imread(data[class_target][choice_arr[i]])\n",
    "\n",
    "        X_train[i] = temp_img\n",
    "        y_labels[i] = choice_arr[i]'''\n",
    "    \n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            temp_img = cv2.imread(data[i][j])\n",
    "            \n",
    "            X_train[4*i + j] = temp_img\n",
    "            y_label[4*i + j] = i\n",
    "            \n",
    "    X_train = (X_train - 127.5)/127.5\n",
    "    return X_train, y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num)/width))\n",
    "    shape = generated_images.shape[1:3]\n",
    "    image = np.zeros((height*shape[0], width*shape[1], img_channels),\n",
    "                     dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = \\\n",
    "            img[:, :, :,]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    styles_folder = os.listdir(path=os.getcwd() + \"\\\\new256_images\\\\\")\n",
    "    num_styles = len(styles_folder)\n",
    "    data = []\n",
    "    for i in range(num_styles):\n",
    "        data.append(glob.glob(os.getcwd() + '\\\\new256_images\\\\' + styles_folder[i] + '\\\\*'))\n",
    "    return data, num_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_another(epochs = 100, BATCH_SIZE = 4, weights = False):\n",
    "\n",
    "    data, num_styles = get_data()\n",
    "    \n",
    "    \n",
    "    generator = build_generator()\n",
    "    discriminator, d_label = build_discriminator(num_styles)\n",
    "    \n",
    "    discriminator.compile(loss=losses[0], optimizer=d_optim)\n",
    "    d_label.compile(loss=losses[1], optimizer=d_optim)\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "    \n",
    "    \n",
    "    if weights:\n",
    "        generator.load_weights(os.getcwd() + '/25.11 gen_weights.h5')\n",
    "        discriminator.load_weights(os.getcwd() + '/25.11 dis_weights.h5')\n",
    "        d_label.load_weights(os.getcwd() + '/25.11 dis_label_weights.h5')\n",
    "        \n",
    "    \n",
    "    dcgan = generator_containing_discriminator(generator, discriminator, d_label)\n",
    "    \n",
    "    \n",
    "    dcgan.compile(loss=losses[0], optimizer=g_optim)\n",
    "    \n",
    "    discriminator.trainable = True\n",
    "    d_label.trainable = True\n",
    "    \n",
    "    \n",
    "    '''(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "    X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))'''\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for index in range(int(15000/BATCH_SIZE)):\n",
    "            noise = np.random.normal(0, 1, (BATCH_SIZE, latent_dim))\n",
    "                \n",
    "            #real_images, real_labels = get_images_classes(batch_size=BATCH_SIZE, data = data)\n",
    "            \n",
    "            label = index % num_styles\n",
    "            \n",
    "            real_images, real_labels = get_images_classes(BATCH_SIZE, data)\n",
    "            \n",
    "            real_images += np.random.normal(size = img_shape, scale= 0.1)\n",
    "            \n",
    "            generated_images = generator.predict(noise)\n",
    "            \n",
    "            if index % 2 == 0:\n",
    "                X = real_images\n",
    "                real_labels = real_labels - 0.1 + np.random.rand(BATCH_SIZE)*0.2\n",
    "                y_classif = keras.utils.to_categorical(np.zeros(BATCH_SIZE) + real_labels, num_styles)\n",
    "                y = 0.8 + np.random.rand(BATCH_SIZE)*0.2\n",
    "                \n",
    "                d_loss = []\n",
    "                d_loss.append(discriminator.train_on_batch(X, y))\n",
    "                discriminator.trainable = False\n",
    "                d_loss.append(d_label.train_on_batch(X, y_classif))\n",
    "                print(\"epoch %d batch %d d_loss : %f, label_loss: %f\" % (epoch, index, d_loss[0], d_loss[1]))\n",
    "                \n",
    "                X = generated_images\n",
    "                y = np.random.rand(BATCH_SIZE) * 0.2\n",
    "                d_loss = discriminator.train_on_batch(X, y)\n",
    "                \n",
    "                print(\"epoch %d batch %d d_loss : %f\" % (epoch, index, d_loss))\n",
    "            else:\n",
    "                X = generated_images\n",
    "                y = np.random.rand(BATCH_SIZE) * 0.2\n",
    "                d_loss = discriminator.train_on_batch(X, y)\n",
    "                \n",
    "                print(\"epoch %d batch %d d_loss : %f\" % (epoch, index, d_loss))\n",
    "                \n",
    "                X = real_images\n",
    "                real_labels = real_labels - 0.1 + np.random.rand(BATCH_SIZE)*0.2\n",
    "                y_classif = keras.utils.to_categorical(np.zeros(BATCH_SIZE) + real_labels, num_styles)\n",
    "                y = 0.8 + np.random.rand(BATCH_SIZE)*0.2\n",
    "                d_loss = []\n",
    "                d_loss.append(discriminator.train_on_batch(X, y))\n",
    "                discriminator.trainable = False\n",
    "                d_loss.append(d_label.train_on_batch(X, y_classif))\n",
    "                \n",
    "                print(\"epoch %d batch %d d_loss : %f, label_loss: %f\" % (epoch, index, d_loss[0], d_loss[1]))\n",
    "            \n",
    "            \n",
    "            noise = np.random.normal(0, 1, (BATCH_SIZE, latent_dim))\n",
    "            \n",
    "            discriminator.trainable = False\n",
    "            d_label.trainable = False\n",
    "            \n",
    "            y_classif = keras.utils.to_categorical(np.zeros(BATCH_SIZE) + 1/num_styles, num_styles)\n",
    "            y = np.random.rand(BATCH_SIZE) * 0.3\n",
    "            \n",
    "            g_loss = dcgan.train_on_batch(noise, [y, y_classif])\n",
    "            \n",
    "            d_label.trainable = True\n",
    "            discriminator.trainable = True\n",
    "            \n",
    "            print(\"epoch %d batch %d g_loss : %f, label_loss: %f\" % (epoch, index, g_loss[0], g_loss[1]))\n",
    "            \n",
    "            if index % 50 == 0:\n",
    "                        image = combine_images(generated_images)\n",
    "                        image = image*127.5+127.5\n",
    "                        cv2.imwrite(\n",
    "                            os.getcwd() + '\\\\generated\\\\epoch%d_%d.png' % (epoch, index), image)\n",
    "                        image = combine_images(real_images)\n",
    "                        image = image*127.5+127.5\n",
    "                        cv2.imwrite(\n",
    "                            os.getcwd() + '\\\\generated\\\\epoch%d_%d_data.png' % (epoch, index), image)\n",
    "                        \n",
    "        if epoch % 5 == 0:\n",
    "            \n",
    "            \n",
    "            date_today = date.today()\n",
    "            \n",
    "            \n",
    "            \n",
    "            month, day = date_today.month, date_today.day\n",
    "            \n",
    "            # Генерируем описание модели в формате json\n",
    "            d_json = discriminator.to_json()\n",
    "            # Записываем модель в файл\n",
    "            json_file = open(os.getcwd() + \"/%d.%d dis_model.json\" % (day, month), \"w\")\n",
    "            json_file.write(d_json)\n",
    "            json_file.close()\n",
    "            \n",
    "            # Генерируем описание модели в формате json\n",
    "            d_l_json = d_label.to_json()\n",
    "            # Записываем модель в файл\n",
    "            json_file = open(os.getcwd() + \"/%d.%d dis_label_model.json\" % (day, month), \"w\")\n",
    "            json_file.write(d_l_json)\n",
    "            json_file.close()\n",
    "            \n",
    "            # Генерируем описание модели в формате json\n",
    "            gen_json = generator.to_json()\n",
    "            # Записываем модель в файл\n",
    "            json_file = open(os.getcwd() + \"/%d.%d gen_model.json\" % (day, month), \"w\")\n",
    "            json_file.write(gen_json)\n",
    "            json_file.close()\n",
    "            \n",
    "            discriminator.save_weights(os.getcwd() + '/%d.%d dis_weights.h5' % (day, month))\n",
    "            d_label.save_weights(os.getcwd() + '/%d.%d dis_label_weights.h5' % (day, month))\n",
    "            generator.save_weights(os.getcwd() + '/%d.%d gen_weights.h5' % (day, month))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 256\n",
    "img_cols = 256\n",
    "img_channels = 3\n",
    "img_shape = (img_rows, img_cols, img_channels)\n",
    "latent_dim = 100\n",
    "filter_size_g = (5,5)\n",
    "filter_size_d = (5,5)\n",
    "d_strides = (2,2)\n",
    "\n",
    "color_mode = 'rgb'\n",
    "\n",
    "losses = ['binary_crossentropy', 'categorical_crossentropy']\n",
    "\n",
    "\n",
    "#g_optim = SGD(lr = 0.001, momentum=0.9, nesterov=True)\n",
    "#d_optim = SGD(lr = 0.00025, momentum=0.9, nesterov=True)\n",
    "g_optim = Adam(0.0002, beta_2 = 0.5)\n",
    "d_optim = Adam(0.0002, beta_2 = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16384)             1654784   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16384)             65536     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 8, 8, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 8, 8, 512)         6554112   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 8, 8, 256)         3277056   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 16, 16, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 32, 32, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 64, 64, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTr (None, 128, 128, 16)      12816     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 128, 128, 16)      64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 128, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTr (None, 256, 256, 8)       3208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 256, 256, 8)       32        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 256, 256, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_9 (Conv2DTr (None, 256, 256, 3)       603       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256, 256, 3)       0         \n",
      "=================================================================\n",
      "Total params: 15,926,963\n",
      "Trainable params: 15,891,139\n",
      "Non-trainable params: 35,824\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 64)      4864      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 256)       819456    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 512)       3277312   \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 512)         6554112   \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 512)         6554112   \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "=================================================================\n",
      "Total params: 17,422,464\n",
      "Trainable params: 17,418,624\n",
      "Non-trainable params: 3,840\n",
      "_________________________________________________________________\n",
      "epoch 0 batch 0 d_loss : 0.890984, label_loss: 2.611388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programming\\Anaconda1\\envs\\NN\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 batch 0 d_loss : 3.677941\n",
      "epoch 0 batch 0 g_loss : 2.812591, label_loss: 2.021804\n",
      "epoch 0 batch 1 d_loss : 1.935898\n",
      "epoch 0 batch 1 d_loss : 0.741040, label_loss: 2.275065\n",
      "epoch 0 batch 1 g_loss : 1.217680, label_loss: 0.612841\n",
      "epoch 0 batch 2 d_loss : 1.555758, label_loss: 2.927476\n",
      "epoch 0 batch 2 d_loss : 0.576666\n",
      "epoch 0 batch 2 g_loss : 1.362889, label_loss: 1.049098\n",
      "epoch 0 batch 3 d_loss : 0.407129\n",
      "epoch 0 batch 3 d_loss : 1.661355, label_loss: 2.790136\n",
      "epoch 0 batch 3 g_loss : 0.874687, label_loss: 0.723506\n",
      "epoch 0 batch 4 d_loss : 0.698503, label_loss: 3.483192\n",
      "epoch 0 batch 4 d_loss : 0.340807\n",
      "epoch 0 batch 4 g_loss : 0.834532, label_loss: 0.677063\n",
      "epoch 0 batch 5 d_loss : 0.868921\n",
      "epoch 0 batch 5 d_loss : 0.804586, label_loss: 4.514564\n",
      "epoch 0 batch 5 g_loss : 1.265388, label_loss: 1.026126\n",
      "epoch 0 batch 6 d_loss : 0.783518, label_loss: 2.903216\n",
      "epoch 0 batch 6 d_loss : 0.826336\n",
      "epoch 0 batch 6 g_loss : 1.072237, label_loss: 0.564925\n",
      "epoch 0 batch 7 d_loss : 0.462696\n",
      "epoch 0 batch 7 d_loss : 0.840555, label_loss: 3.040557\n",
      "epoch 0 batch 7 g_loss : 3.159909, label_loss: 1.938664\n",
      "epoch 0 batch 8 d_loss : 0.750308, label_loss: 2.834229\n",
      "epoch 0 batch 8 d_loss : 1.229064\n",
      "epoch 0 batch 8 g_loss : 3.731395, label_loss: 2.658054\n",
      "epoch 0 batch 9 d_loss : 1.433921\n",
      "epoch 0 batch 9 d_loss : 0.725755, label_loss: 3.014248\n",
      "epoch 0 batch 9 g_loss : 3.752099, label_loss: 2.274845\n",
      "epoch 0 batch 10 d_loss : 1.249645, label_loss: 3.002583\n",
      "epoch 0 batch 10 d_loss : 1.345886\n",
      "epoch 0 batch 10 g_loss : 3.342218, label_loss: 2.197862\n",
      "epoch 0 batch 11 d_loss : 1.734123\n",
      "epoch 0 batch 11 d_loss : 1.103396, label_loss: 4.783787\n",
      "epoch 0 batch 11 g_loss : 3.532306, label_loss: 2.352751\n",
      "epoch 0 batch 12 d_loss : 1.410740, label_loss: 2.033005\n",
      "epoch 0 batch 12 d_loss : 1.858411\n",
      "epoch 0 batch 12 g_loss : 3.018040, label_loss: 2.045716\n",
      "epoch 0 batch 13 d_loss : 1.479702\n",
      "epoch 0 batch 13 d_loss : 0.526101, label_loss: 2.715168\n",
      "epoch 0 batch 13 g_loss : 1.468115, label_loss: 1.242763\n",
      "epoch 0 batch 14 d_loss : 0.681376, label_loss: 3.978224\n",
      "epoch 0 batch 14 d_loss : 0.501277\n",
      "epoch 0 batch 14 g_loss : 0.780313, label_loss: 0.617777\n",
      "epoch 0 batch 15 d_loss : 0.832978\n",
      "epoch 0 batch 15 d_loss : 0.662767, label_loss: 3.350384\n",
      "epoch 0 batch 15 g_loss : 1.374168, label_loss: 1.097105\n",
      "epoch 0 batch 16 d_loss : 0.619416, label_loss: 4.057438\n",
      "epoch 0 batch 16 d_loss : 0.785851\n",
      "epoch 0 batch 16 g_loss : 2.031680, label_loss: 1.857918\n",
      "epoch 0 batch 17 d_loss : 0.793394\n",
      "epoch 0 batch 17 d_loss : 0.552716, label_loss: 3.553845\n",
      "epoch 0 batch 17 g_loss : 1.659500, label_loss: 1.248423\n",
      "epoch 0 batch 18 d_loss : 0.822994, label_loss: 4.664506\n",
      "epoch 0 batch 18 d_loss : 0.402560\n",
      "epoch 0 batch 18 g_loss : 1.520365, label_loss: 0.461101\n",
      "epoch 0 batch 19 d_loss : 0.801384\n",
      "epoch 0 batch 19 d_loss : 0.422413, label_loss: 3.647184\n",
      "epoch 0 batch 19 g_loss : 1.124949, label_loss: 0.461449\n",
      "epoch 0 batch 20 d_loss : 0.583260, label_loss: 4.648105\n",
      "epoch 0 batch 20 d_loss : 0.356939\n",
      "epoch 0 batch 20 g_loss : 0.838802, label_loss: 0.709426\n",
      "epoch 0 batch 21 d_loss : 0.338687\n",
      "epoch 0 batch 21 d_loss : 0.407487, label_loss: 1.991859\n",
      "epoch 0 batch 21 g_loss : 0.946768, label_loss: 0.933278\n",
      "epoch 0 batch 22 d_loss : 0.715051, label_loss: 3.320470\n",
      "epoch 0 batch 22 d_loss : 0.654696\n",
      "epoch 0 batch 22 g_loss : 0.979104, label_loss: 0.962878\n",
      "epoch 0 batch 23 d_loss : 0.459690\n",
      "epoch 0 batch 23 d_loss : 0.558978, label_loss: 2.217589\n",
      "epoch 0 batch 23 g_loss : 0.650882, label_loss: 0.622703\n",
      "epoch 0 batch 24 d_loss : 0.802293, label_loss: 3.633367\n",
      "epoch 0 batch 24 d_loss : 1.527431\n",
      "epoch 0 batch 24 g_loss : 0.806380, label_loss: 0.635176\n",
      "epoch 0 batch 25 d_loss : 0.704079\n",
      "epoch 0 batch 25 d_loss : 0.403212, label_loss: 4.035698\n",
      "epoch 0 batch 25 g_loss : 1.361408, label_loss: 0.827878\n",
      "epoch 0 batch 26 d_loss : 0.558023, label_loss: 2.543997\n",
      "epoch 0 batch 26 d_loss : 1.328564\n",
      "epoch 0 batch 26 g_loss : 2.084309, label_loss: 1.796885\n",
      "epoch 0 batch 27 d_loss : 1.105038\n",
      "epoch 0 batch 27 d_loss : 0.616188, label_loss: 6.361230\n",
      "epoch 0 batch 27 g_loss : 1.757542, label_loss: 1.403970\n",
      "epoch 0 batch 28 d_loss : 0.558240, label_loss: 5.790011\n",
      "epoch 0 batch 28 d_loss : 0.550553\n",
      "epoch 0 batch 28 g_loss : 0.730340, label_loss: 0.445137\n",
      "epoch 0 batch 29 d_loss : 0.404504\n",
      "epoch 0 batch 29 d_loss : 0.368563, label_loss: 5.936755\n",
      "epoch 0 batch 29 g_loss : 1.390428, label_loss: 1.293190\n",
      "epoch 0 batch 30 d_loss : 0.384683, label_loss: 4.594255\n",
      "epoch 0 batch 30 d_loss : 3.261708\n",
      "epoch 0 batch 30 g_loss : 2.819899, label_loss: 2.765434\n",
      "epoch 0 batch 31 d_loss : 1.628162\n",
      "epoch 0 batch 31 d_loss : 0.765677, label_loss: 4.603161\n",
      "epoch 0 batch 31 g_loss : 0.853771, label_loss: 0.827935\n",
      "epoch 0 batch 32 d_loss : 0.312907, label_loss: 4.087281\n",
      "epoch 0 batch 32 d_loss : 0.388231\n",
      "epoch 0 batch 32 g_loss : 0.525513, label_loss: 0.518860\n",
      "epoch 0 batch 33 d_loss : 0.503025\n",
      "epoch 0 batch 33 d_loss : 0.494386, label_loss: 5.153328\n",
      "epoch 0 batch 33 g_loss : 0.969594, label_loss: 0.904812\n",
      "epoch 0 batch 34 d_loss : 0.670974, label_loss: 4.103697\n",
      "epoch 0 batch 34 d_loss : 0.643801\n",
      "epoch 0 batch 34 g_loss : 1.322337, label_loss: 1.307080\n",
      "epoch 0 batch 35 d_loss : 0.656600\n"
     ]
    }
   ],
   "source": [
    "train_another(1000, 16, weights = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_moment = datetime.now()\n",
    "\n",
    "day, month = str(current_moment.day), str(current_moment.month)\n",
    "hour, minute = str(current_moment.hour), str(current_moment.minute)\n",
    "\n",
    "with open(os.getcwd() + '/Changes.txt', 'a') as f:\n",
    "    clock =  (day + '.' + month + '    ' + hour + ':' + minute)\n",
    "    f.write(clock + '...........g_optim -> 0.0003, d_optim->0.0004' + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
